{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import mean_sst\n",
    "import mean_sst_dask\n",
    "import requests as rq\n",
    "import json\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server, Daten und SST\n",
    "\n",
    "Die erste Version eines `docker-compose.yml` ist jetzt fertig und verbindet die API mit dem SST Prozess. Au√üerdem funktioniert jetzt das Job Mangement des Servers, um diesen Prozess zu starten.\n",
    "\n",
    "### Durchlauf eines Jobs\n",
    "\n",
    "1. Zuerst startet man den Server √ºber:\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "(in dem Verzeichnis, wo dieses Notebook liegt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ein Job JSON erstellen:\\\n",
    "Ein Beispiel JSON f√ºr einen SST Job (freundlicher Weise vom Server Team zur Verf√ºgung gestellt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testjob = {\n",
    "  \"title\": \"Example Title\",\n",
    "  \"description\": \"Example Description\",\n",
    "  \"process\": {\n",
    "    \"process_graph\": {\n",
    "      \"loadcollection1\": {\n",
    "        \"process_id\": \"load_collection\",\n",
    "        \"arguments\": {\n",
    "          \"timeframe\" : [\"01-12-1981 00:00:00\",\"30-12-1981 00:00:00\",\"%d-%m-%Y %H:%M:%S\"],\n",
    "          \"DataType\": \"SST\"\n",
    "        }\n",
    "        },\n",
    "        \"SST\": {\n",
    "        \"process_id\": \"mean_sst\",\n",
    "        \"arguments\": {\n",
    "          \"data\":{\n",
    "              \"from_node\": \"loadcollection1\"\n",
    "          },\n",
    "          \"timeframe\":[\"1981-12-01\",\"1981-12-17\"],\n",
    "          \"bbox\":[-999,-999,-999,-999]\n",
    "          }\n",
    "        },\n",
    "        \"save\":{\n",
    "            \"process_id\": \"save_result\",\n",
    "            \"arguments\":{\n",
    "                \"SaveData\":{\n",
    "                    \"from_node\":\"SST\"\n",
    "                },\n",
    "                \"Format\": \"netcdf\"\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Den Job mittels HTTP Post an den jobs Endpoint schicken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Den Testdaten an /jobs Posten\n",
    "rq.post(\"http://localhost/api/v1/jobs\", json=testjob, headers={\"Content-Type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Die Id des Jobs erfragen, in dem eine GET Anfrage an den /jobs Endpoint gestellt wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = rq.get(\"http://localhost/api/v1/jobs\")\n",
    "rjson = j.json()\n",
    "# Die Id des neusten Job abspeichern f√ºr sp√§tere Verwendung\n",
    "job_id = rjson['jobs'][-1]['id']\n",
    "rjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Den Job ausf√ºhren √ºber eine POST Anfrage an den results Endpoint des Jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rq.post(\"http://localhost/api/v1/jobs/\" + job_id + \"/results\" , json=None, headers={\"Content-Type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Verlauf des Jobs im Server:\n",
    "\n",
    "Was im der Server im Hintergund macht: \n",
    "\n",
    "```bash\n",
    "frontend_1        | 172.21.0.1 - - [20/Jan/2021 12:14:29] \"POST /api/v1/jobs/051a9bc0-5b19-11eb-a9c8-0242ac150006/results HTTP/1.1\" 204 -\n",
    "frontend_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"GET /jobRunning/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"POST /doJob/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "...\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"POST /doJob/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:42] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:47] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:52] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:57] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "frontend_1        | 172.21.0.5 - - [20/Jan/2021 12:16:57] \"POST /takeData/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "```\n",
    "- Zuerst √ºbergibt der `frontend` container, den Job an den `database` container. \\\n",
    "- Dannach wird vom `database` container der entsprechende Datacube vorbereitet. \\\n",
    "- Dann wird √ºbernimmt der `sst` container und berechnet den SST f√ºr den ausgew√§hlten Zeitraum. \\\n",
    "- Das Ergebnis wird dann dem `/takeData` Endpoint des `frontend` containers √ºbergeben und das Ergebnis zum Download bereitgestellt.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Den Downloadlink √ºber eine GET Anfrage an /jobs results bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rq.get(\"http://localhost/api/v1/jobs/\" + job_id + \"/results\" )\n",
    "dl = res.json()[\"assets\"]\n",
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Link im Browser eingeben und Ergebnis als netCDF herunterladen: http://localhost:80/download/< Euer Downloadlink>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin = xr.open_dataset('../demodata/25ff212c-5b2c-11eb-a863-0242ac160005.nc') # Hier ggf. euren eigenen Pfad zum DOwnload einf√ºgen\n",
    "fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( 9. Mit `docker-compose down -v` die lokale Umgebung aufr√§umen)\n",
    "\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask SST Laufzeitanalyse\n",
    "---\n",
    "*von Alexia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\" (https://docs.dask.org/en/latest/delayed.html)\n",
    "-  Ein einfacher Weg Code, der nicht voneinander anh√§ngt, parallel auszuf√ºhren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dask divides arrays into many small pieces, called chunks, each of which is presumed to be small enough to fit into memory.\" \n",
    "\"If your chunks are too small, queueing up operations will be extremely slow, because Dask will translate each operation into a huge number of operations mapped across chunks. Computation on Dask arrays with small chunks can also be slow, because each operation on a chunk has some fixed overhead from the Python interpreter and the Dask task executor. Conversely, if your chunks are too big, some of your computation may be wasted, because Dask only computes results one chunk at a time.\" (http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance)\n",
    "-  Erlaubt es ein Dataset in St√ºcke zu teilen, auf denen Operationen parallel angewendet werden k√∂nnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dask Cluster'''\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monate, kleines Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed und chunks => keine Parallelisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mit Dask delayed, ohne chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst_dask.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dauert l√§nger, da dask delayed overhead erzeugt, der code an sich damit aber nicht parallelisiert werden kann, da er aufeinander aufbaut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "das verwenden wir. \"auto\" findet die beste chunk-gr√∂√üe automatisch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"lat\": 200, \"lon\": 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"lat\": 200, \"lon\": 400})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schlecht gew√§hlte chunk-gr√∂√üe, hier manuell vorgegeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"lat\": \"auto\", \"lon\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"lat\": \"auto\", \"lon\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunken von lat und lon an sich scheint nicht ideal zu sein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mit Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem, dieser fall verbraucht zu viel memory: distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst_dask.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monate, kein Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed und chunks => keine Parallelisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Abstand zwischen den Laufzeiten von dem Fall ohne Parallelsisierung und dem Fall chunks={\"time\": \"auto\"} nimmt mit zunehmender gr√∂√üe des datasets zu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen \n",
    "\n",
    "Inzwischen gibt es zwei fertige Github Actions, im [Testrepo](https://github.com/GeoSoftII2020-21/TestRepo/actions).\n",
    "So soll auch das One Click Demo Szenario aus dem Pflichtenheft umgesetzt werden.\n",
    "\n",
    "###### [1. Pytest Workflow](https://github.com/GeoSoftII2020-21/TestRepo/actions?query=workflow%3A%22Pytest+Workflow%22)\n",
    "\n",
    "Diese Action soll alle Submodule des Projekts ihre Unittests mit Pytest ausf√ºhren lassen. Dazu werden zuerst alle Teilprojekte geupdated. Dann wird auf in einem zweiten Schritt Pytest √ºber alle gesammelten Unitests ausgef√ºhrt.\n",
    "\n",
    "üöß *Momentan gibt es noch ein Problem mit den Testdatens√§tzen, die f√ºr manche Tests verwendet wurden. Da diese, aufgrund ihrer Gr√∂√üe, nicht auf Github liegen, schl√§gt die Action bis jetzt noch fehl. An einer L√∂sung wird gearbeitet.*\n",
    "\n",
    "##### [2. Backendvalidator](https://github.com/GeoSoftII2020-21/TestRepo/actions?query=workflow%3ABackend-Validator)\n",
    "\n",
    "Diese Action f√ºhrt den openeo Backendvalidator(s. Demo II f√ºr die lokale Ausf√ºhrung) nun √ºber Github aus. Dabei wird das openeo Backend Validator Repository geklont und der validate befehl auf die den momentanen Stand der API ausgef√ºhrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_owner",
   "language": "python",
   "name": "product_owner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
